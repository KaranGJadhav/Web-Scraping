{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web Scraping is the process of gathering necessary information from internet.It is also termed as Web Harvesting or Web Extraction.\n",
    "Copy pasting the data from wikipedia page to your local machine is also a kind of web scraping but it's a manual task. Web Scraping term is used when we automate a task of gathering the unorganized data into the format one needs. \n",
    "\n",
    "Some websites offer datasets that are downloadable in CSV or xls format and some expose their data to an API. But some websites wont offer us this facility. \n",
    "\n",
    "\n",
    "Example:If we want to do a Review analysis of different products sold on e-commerce website, and want to gather information like the \"Comments\" given my the customers, \"Ratings\" given , \"Price\" of the product, \"Vendor\" name etc, and the following e-commerce website does'nt allow us to download data in the format needed for analysis.\n",
    "\n",
    "\n",
    "In such scenario copy pasting each and every information we need is not logical and is time consuming. Web scraping is a technique that lets us make use of programming to automate the task. We can write some code that looks at the e-commerce site, grabs just the data we want to work with and outputs it in the format we need.\n",
    "\n",
    "\n",
    "Web Scraping includes following steps.\n",
    "\n",
    "1. Find the URL that you want to scrape(E-Commerce Website,Weather Forecast Website,Travel Website etc).\n",
    "2. Inspecting the Page(Finding and understanding different Elements on the web page which holds the important data which we    need , like the div,p,span tags on an HTML page).\n",
    "3. Find the data you want to extract(Necessary information which we need).\n",
    "4. Write the code(Program the code to extract the necessary data needed from the Elements analysed).\n",
    "5. Run the code and extract the data.\n",
    "6. Store the data in the required format(As per the need store the data, say in CSV or xls format).\n",
    "\n",
    "When we do web scraping, we bascially write a code that sends a request to the server that host's the page we specified. Our code downloads the source code of the page, further it extracts whatever content we’ve instructed it to extract.\n",
    "\n",
    "\n",
    "For example, if we want to get all of information inside p tags from a website, we could write some code that can extract the information inside the p tags.\n",
    "Firstly, code would request the content from the server hosting the page and download it. Then it would go through the page’s HTML looking for the p tags. Whenever it found an p tags, it would copy whatever text is inside the tag, and output it in whatever format we specified.\n",
    "\n",
    "Depending on our requirement we may be looking for specific tags for the information on HTML page, so accordingly we will be programming our code. \n",
    "\n",
    "Python is the most widely used programming language for web scraping because it comes packaged with different libraries which ease our task to extract the data from the website.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are few basic libraries which help us with the task.\n",
    "\n",
    "1. \"requests\" libraries help's in getting the source code from the website.Library will make a GET request to a web server, which will download the HTML contents of a given web page.\n",
    "\n",
    "Example: x=requests.get(\"http://weatherinfo.io/simplewebinfo.html\")\n",
    "\n",
    "\n",
    "Note: This is an hypothetical website used just to demonstrate.\n",
    "\n",
    "We can print out the HTML content of the page using the content property.\n",
    "\n",
    "content=x.content\n",
    "\n",
    "print(content)\n",
    "![image](https://user-images.githubusercontent.com/51224958/94894551-39297580-04a7-11eb-9761-2ca709023b88.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. BeautifulSoup library  \n",
    "\n",
    "We can use the BeautifulSoup library to parse the above downloaded HTML document, and extract the text from the p tag. \n",
    "We first have to import the library, create an instance of the BeautifulSoup class to parse the document.\n",
    "\n",
    "from bs4 import BeautifulSoup -- Import the library.\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(content) -- Create an instance of the BeautifulSoup library and pass the content as an argument.\n",
    "\n",
    "print(soup)\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/51224958/94895020-26fc0700-04a8-11eb-9479-379fbcb4a71f.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have illustrated Web Scraping using a simple example.\n",
    "\n",
    "\n",
    "In the below example we are extracting the information of the best selling gift cards on Amazon Website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Necesssay Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# URL to Scrape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "URL which we are scraping is \"'https://www.amazon.in/gp/bestsellers/gift-cards/ref=zg_bs_pg_'+str(pageNo)+'?ie=UTF8&pg='+str(pageNo)\"\n",
    "\n",
    "As you can see from the URL it has a argument \"pageno\". Passing the argument we will be able to access information about gift cards on each page. To get the information of gift cards on all pages we can loop through all the pages and get our necessary data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect the page "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the parent tag under which all the data you need will reside. The data that you are going to extract is\n",
    "\n",
    "1. Gift Card Name.\n",
    "2. Rating\n",
    "3. Customers Rated\n",
    "4. Price\n",
    "\n",
    "Amazon Link For Reference:\n",
    "\n",
    "https://www.amazon.in/gp/bestsellers/gift-cards/ref=zg_bs_gift-cards_home_all?pf_rd_p=3ca9e017-7be2-4b78-8c8d-9a28094b2453&pf_rd_s=center-1&pf_rd_t=2101&pf_rd_i=home&pf_rd_m=A1VBAL9TL5WCBF&pf_rd_r=TXZM2GV0T240F3ZJ3A5N&pf_rd_r=TXZM2GV0T240F3ZJ3A5N&pf_rd_p=3ca9e017-7be2-4b78-8c8d-9a28094b2453\n",
    "\n",
    "The below image shows the parent tag which holds all the gift card details. When you hover over the tag you can see the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/51224958/94895147-688cb200-04a8-11eb-9826-bf3ca522c5f1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect Elements which hold the necessary information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have identified the parent tag, we need to inspect the elements that hold the information such as Price, Gift Card Name, Customer Rating, Count of total customers rated etc.\n",
    "\n",
    "\n",
    "Now say we need to know the tag which holds information on \"Price\" of the gift card. We can right click on price tag, go to inspect as shown in image below. On right hand side we can see the tag holding the information on the price of the product.\n",
    "\n",
    "Similary we can do it for \"Gift Card Name\", \"Rating\" etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/51224958/94895438-0b453080-04a9-11eb-8276-25c140f0ce49.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to extract the necessary data from the HTML Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that we know the tags which hold the necessary information we can program the code using Python to extract necessary information from the identified tags.\n",
    "\n",
    "We will be writing a function which will be looped to run for the number of pages for which the user what's the information.\n",
    "\n",
    "get_info -- Function used.\n",
    "\n",
    "lambda -- To flatten the output from the nested list output.\n",
    "\n",
    "Description of the methods used.\n",
    "\n",
    "find_all --  To find all the instances of an element.\n",
    "\n",
    "find -- Find the first occurance of the element.\n",
    "\n",
    "Variables Used:\n",
    "\n",
    "divalls[] -- Array Variable used to store the entire nested list.\n",
    "\n",
    "divall[] -- Array Variable used to store the list of lists of information for each iteration of the loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_pages = 2\n",
    "\n",
    "def get_info(pageNo):\n",
    "    #Get the source code from the web server(Download the HTML contents of the web page)\n",
    "    x = requests.get('https://www.amazon.in/gp/bestsellers/gift-cards/ref=zg_bs_pg_'+str(pageNo)+'?ie=UTF8&pg='+str(pageNo))\n",
    "    content = x.content\n",
    "    #Parse the HTML document downloaded, using Beautiful Soup library\n",
    "    soup = BeautifulSoup(content)\n",
    "    #Create an empty array to store the required information from all the HTML elements.\n",
    "    divalls = []\n",
    "    #findall method, which will find all the instances of a <div>tag with class attribute \"a-section a-spacing-none aok-relative\" on a page.\n",
    "    #Note that find_all and findall returns a list, so we’ll have to loop through, or use list indexing, it to extract text.\n",
    "    for d in soup.findAll('div', attrs={'class':'a-section a-spacing-none aok-relative'}):\n",
    "        # Find the first occurance of <span> tag with clas attribute \"zg-text-center-align\"\n",
    "        name = d.find('span', attrs={'class':'zg-text-center-align'})\n",
    "        #find_all method, which will find all the instances of a <img> tag which includes the name of the gift card.\n",
    "        n = name.find_all('img', alt=True)\n",
    "        #span tag with attribute \"a-icon-alt\" below includes the rating given by the customers\n",
    "        rating = d.find('span', attrs={'a-icon-alt'})\n",
    "        # a tag with attribute \"class':'a-size-small a-link-normal\" includes the count of total reviews or ratings given by the customers for specific gift card. \n",
    "        users_rated = d.find('a', attrs={'class':'a-size-small a-link-normal'})\n",
    "        #span tag below includes the information on the price of the specific gift card \n",
    "        price = d.find('span', attrs={'p13n-sc-price'})\n",
    "\n",
    "        divall=[]\n",
    "\n",
    "        if name is not None:\n",
    "            divall.append(n[0]['alt'])\n",
    "        else:\n",
    "            divall.append(\"unknown-card\")\n",
    "\n",
    "        if rating is not None:\n",
    "            divall.append(rating.text)\n",
    "        else:\n",
    "            divall.append('-1')\n",
    "\n",
    "        if users_rated is not None:\n",
    "            divall.append(users_rated.text)\n",
    "        else:\n",
    "            divall.append('0')     \n",
    "\n",
    "        if price is not None:\n",
    "            divall.append(price.text)\n",
    "        else:\n",
    "            divall.append('0')\n",
    "        divalls.append(divall)    \n",
    "    return divalls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing the data in required CSV format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "for i in range(1, no_pages+1):\n",
    "    output.append(get_info(i))\n",
    "#Since the output will be a nested list, you would first flatten the list and then pass it to the DataFrame\n",
    "#lambda is used to flatten the output.\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "#Dataframe is created out of flattened output and then convereted into a csv file.\n",
    "df = pd.DataFrame(flatten(output),columns=['Gift Card','Rating','Customers Rated', 'Price'])\n",
    "df.to_csv('amazon_gift_cards.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gift Card</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Customers Rated</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amazon Pay eGift Card</td>\n",
       "      <td>4.5 out of 5 stars</td>\n",
       "      <td>27,441</td>\n",
       "      <td>₹ 100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Google Play Gift Code - Digital Voucher</td>\n",
       "      <td>4.3 out of 5 stars</td>\n",
       "      <td>5,614</td>\n",
       "      <td>₹ 1,500.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amazon Pay eGift Card</td>\n",
       "      <td>4.5 out of 5 stars</td>\n",
       "      <td>27,441</td>\n",
       "      <td>₹ 10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Amazon Pay eGift Card</td>\n",
       "      <td>4.5 out of 5 stars</td>\n",
       "      <td>27,441</td>\n",
       "      <td>₹ 100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Amazon Pay eGift Card</td>\n",
       "      <td>4.5 out of 5 stars</td>\n",
       "      <td>27,441</td>\n",
       "      <td>₹ 100.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Gift Card              Rating  \\\n",
       "0                    Amazon Pay eGift Card  4.5 out of 5 stars   \n",
       "1  Google Play Gift Code - Digital Voucher  4.3 out of 5 stars   \n",
       "2                    Amazon Pay eGift Card  4.5 out of 5 stars   \n",
       "3                    Amazon Pay eGift Card  4.5 out of 5 stars   \n",
       "4                    Amazon Pay eGift Card  4.5 out of 5 stars   \n",
       "\n",
       "  Customers Rated       Price  \n",
       "0          27,441    ₹ 100.00  \n",
       "1           5,614  ₹ 1,500.00  \n",
       "2          27,441     ₹ 10.00  \n",
       "3          27,441    ₹ 100.00  \n",
       "4          27,441    ₹ 100.00  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('amazon_gift_cards.csv').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot directly use the data above for analysis. It is just a raw data that needs feature engineering so that it can be used for data analysis. \n",
    "\n",
    "Article gives basic idea about the Web Scraping.\n",
    "\n",
    "Note:Feature Engineering part is not in scope of this article."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
